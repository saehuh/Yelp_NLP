{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import seaborn as sns\n",
    "from multiprocessing import Pool\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('data/yelp_dataset/')\n",
    "assert path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset_User_Agreement.pdf           yelp_academic_dataset_review.json\r\n",
      "balanced_simplified_reviews_full.pkl yelp_academic_dataset_tip.json\r\n",
      "yelp_academic_dataset_business.json  yelp_academic_dataset_user.json\r\n",
      "yelp_academic_dataset_checkin.json   yelp_reviews_simple_8021122.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_simplified_reviews = pd.read_pickle(path/'balanced_simplified_reviews_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>This actually used to be one of my favorite ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Decent food. Fishermen lobster or even congee ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Pros: Fun atmosphere, great for people watchin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>I love bookstores, and I love to spend some ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>I passed Five Guys, In-N-Out, Carl's Jr, and S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021116</th>\n",
       "      <td>Fricken unbelievable, I ordered 2 space heater...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021117</th>\n",
       "      <td>Solid American food with a southern comfort fl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021118</th>\n",
       "      <td>I'm honestly not sure how I have never been to...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021119</th>\n",
       "      <td>Food was decent but I will say the service too...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8021120</th>\n",
       "      <td>Oh yeah! Not only that the service was good, t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3175356 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  stars\n",
       "31       This actually used to be one of my favorite ho...      1\n",
       "62       Decent food. Fishermen lobster or even congee ...      1\n",
       "77       Pros: Fun atmosphere, great for people watchin...      1\n",
       "85       I love bookstores, and I love to spend some ti...      1\n",
       "86       I passed Five Guys, In-N-Out, Carl's Jr, and S...      1\n",
       "...                                                    ...    ...\n",
       "8021116  Fricken unbelievable, I ordered 2 space heater...      0\n",
       "8021117  Solid American food with a southern comfort fl...      2\n",
       "8021118  I'm honestly not sure how I have never been to...      4\n",
       "8021119  Food was decent but I will say the service too...      2\n",
       "8021120  Oh yeah! Not only that the service was good, t...      4\n",
       "\n",
       "[3175356 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_simplified_reviews['stars'] = balanced_simplified_reviews['stars'] - 1\n",
    "balanced_simplified_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing tokenziser and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(balanced_simplified_reviews['stars'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56aff45e9365473cbbefb9b888fdb136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-uncased': 512,\n",
       " 'bert-large-uncased': 512,\n",
       " 'bert-base-cased': 512,\n",
       " 'bert-large-cased': 512,\n",
       " 'bert-base-multilingual-uncased': 512,\n",
       " 'bert-base-multilingual-cased': 512,\n",
       " 'bert-base-chinese': 512,\n",
       " 'bert-base-german-cased': 512,\n",
       " 'bert-large-uncased-whole-word-masking': 512,\n",
       " 'bert-large-cased-whole-word-masking': 512,\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad': 512,\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad': 512,\n",
       " 'bert-base-cased-finetuned-mrpc': 512,\n",
       " 'bert-base-german-dbmdz-cased': 512,\n",
       " 'bert-base-german-dbmdz-uncased': 512,\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1': 512,\n",
       " 'TurkuNLP/bert-base-finnish-uncased-v1': 512,\n",
       " 'wietsedv/bert-base-dutch-cased': 512}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=8):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(df):\n",
    "    df['text_len'] = df['text'].apply(lambda t : len(tokenizer.encode(t, max_length=config.MAX_LENGTH,truncation=True)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 29.4 s, total: 39.5 s\n",
      "Wall time: 17min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "balanced_simplified_reviews = parallelize_dataframe(balanced_simplified_reviews, count_tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe09ed95b50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(balanced_simplified_reviews['text_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(Dataset):\n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews, self.targets,  self.tokenizer, self.max_len = reviews.to_numpy(), targets.to_numpy(), tokenizer, max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        review = self.reviews[item]\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review': review,\n",
    "            'input_ids': tokens['input_ids'].flatten(),\n",
    "            'attention_mask': tokens['attention_mask'].flatten(),\n",
    "            'target': torch.tensor(self.targets[item]).long()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2857820, 3), (158768, 3), (158768, 3))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(balanced_simplified_reviews, test_size=0.1, random_state=RANDOM_SEED, \n",
    "                                     stratify=balanced_simplified_reviews.stars.values)\n",
    "valid_df, test_df = train_test_split(test_df, test_size=0.5, random_state=RANDOM_SEED, stratify=test_df.stars.values)\n",
    "train_df.shape, test_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the distribution is even. We used stratify so it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdc926b83d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(valid_df['stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_length, batch_size):\n",
    "    ds = YelpDataset(df['text'], df['stars'], tokenizer, max_length)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=multiprocessing.cpu_count() - 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "\n",
    "train_dl = create_data_loader(train_df, tokenizer, config.MAX_LENGTH, BATCH_SIZE)\n",
    "test_dl = create_data_loader(test_df, tokenizer, config.MAX_LENGTH, BATCH_SIZE)\n",
    "valid_dl = create_data_loader(valid_df, tokenizer, config.MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  101,  1109,  2166,  3189,  1108,  2385, 14739,   117,  1463,  5953,\n",
       "         1110,  1136,   109,   124,  1112,  9129,   117,  1133,   109,   124,\n",
       "          119,  4850,  1105,  1146,   119, 18491,  1174,  1103,   109,   123,\n",
       "        20049, 25754,  6462,  1105,  1122,  1108,  2385,  4296,  1107,  3849,\n",
       "          119,  2907,  2802,   170,  6707, 16091, 19557,  2728,   117,  1134,\n",
       "         1108, 11858,   117,  1105,  1103,   158,  3842,  1302,  5412,  1513,\n",
       "         1573,  4455,  1111,   109,   124,   119,  4850,  1108,  2785, 16047,\n",
       "          119, 20394,  5773,  1183,   190,  3842,  1185, 27267,  1114,   170,\n",
       "         4778,  9304, 12858,   119,  1370,  1115,  3945,  1463,   117,  1122,\n",
       "         1156,   112,  1396,  1151,  1618,  1191,  1175,  1127,  1199, 27001,\n",
       "         1116,  1107,  1103, 13128,   119,  3982,  1500,  1118,  2546,  1115,\n",
       "         1147,  7253,  1733,  1132,  2785,  1363,   119,  1448,  1111,   109,\n",
       "          122,   119,  1851,   117,  1105,  1175,   112,   188,   170,  2239,\n",
       "         1191,  1128,  1243,   124,  1104,  1172,   119,  3100,  1660,  1115,\n",
       "          170,  2222,  1397,  1159,   199,   106, 20515,  1104,  1103,  2984,\n",
       "         1110,  1253,  1217,  4482,  1174,   117,  1177,  1122,   112,   188,\n",
       "         1253,   170,  2113, 22726,   119, 20045,  1104,  6092,  1116,  1105,\n",
       "         2343, 24263,  1108,  1632,   119,  2066,  1444,  1106,  1250,  1113,\n",
       "         1103,  1168,  1877,   119,  4299,   122,   118,  1214,  3443,  5467,\n",
       "         1253,  1107,  2629,   117,  1133,  9424,  1106,  2367,  1165,  1152,\n",
       "         1127,  3830,  1103,  4166,   119,  1753,  1612,  1191,  1639,  1209,\n",
       "         1176,  1142,   117,  1133,  1175,  1127,   170,  1974,  1104,  2546,\n",
       "         1213,  1115,  4685,  5793,  1106,  5996,  1105,  8698,  1207,  2094,\n",
       "         4454,   119,  8540,  1977,  1106,  1165,  1152,   112,  1231,  3106,\n",
       "         4482,  1174,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_record = next(iter(train_dl))\n",
    "sample_record['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_record['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(config.PRE_TRAINED_MODEL_NAME, num_labels = NUM_CLASSES, \n",
    "                                                      output_attentions = False, output_hidden_states = False)\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-3},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 124)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([p for n, p in param_optimizer if not any(nd in n for nd in no_decay)]), len([p for n, p in param_optimizer if any(nd in n for nd in no_decay)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW Adam algorithm with weight decay fix\n",
    "optimizer = AdamW(optimizer_parameters, \n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "total_steps = len(train_dl) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "def save_model(model, tokenizer, output_dir = path/'model_save'):\n",
    "\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    else:\n",
    "        shutil.rmtree(output_dir)\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    torch.save(model, output_dir/'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_batch_data(batch):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    input_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['target'].to(device)\n",
    "    return input_ids, input_mask, labels\n",
    "\n",
    "def eval_fn():\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(valid_dl, total=len(valid_dl)):\n",
    "        input_ids, input_mask, labels = grab_batch_data(batch)\n",
    "        with torch.no_grad():        \n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # token_type_ids is the same as the \"segment ids\", which \n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            (loss, logits) = model(input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=input_mask,\n",
    "                                   labels=labels)\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences, and\n",
    "            # accumulate it over all batches.\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "            \n",
    "    avg_val_accuracy = total_eval_accuracy / len(valid_dl)\n",
    "    avg_val_loss = total_eval_loss / len(valid_dl)\n",
    "    print(f\"  Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "    return {\n",
    "        'Accuracy': avg_val_accuracy,\n",
    "        'Validation Loss': avg_val_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5231a5c28d83477c9db0439842831c92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bef6ea42a74ae9ab7fa1befefa2726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178614.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/Users/saehuh/opt/anaconda3/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# training_stats = []\n",
    "# best_accuracy = 0\n",
    "\n",
    "for epoch in tqdm(range(0, EPOCHS), total=EPOCHS):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    train_loss_set = []\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0 \n",
    "    \n",
    "    for step, batch in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "        # Unpack the inputs from dataloader\n",
    "        input_ids, input_mask, labels = grab_batch_data(batch)\n",
    "        # Clear ou the gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Forward pass\n",
    "        loss = model(input_ids, token_type_ids=None, attention_mask=input_mask, labels=labels)\n",
    "        train_loss_set.append(loss.item())\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        scheduler.step()\n",
    "        \n",
    "#         if step % 2000 == 0:\n",
    "#             print(f'{step}: Loss: {total_train_loss / (step + 1)}\\r')\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_train_loss / len(train_dl)\n",
    "    print(\"\")\n",
    "    print(f\"  Average training loss: {avg_train_loss}\")\n",
    "    stats_info = eval_fn()\n",
    "    stats_info['epoch'], stats_info['Average training loss'] = epoch, avg_train_loss\n",
    "    training_stats.append(stats_info)\n",
    "    save_model(model, tokenizer, output_dir=path/f'output_dir_{epoch}')\n",
    "    if stats_info['Accuracy'] > best_accuracy:\n",
    "        save_model(model, tokenizer, output_dir=path/f'output_dir_best')\n",
    "        best_accuracy = stats_info['Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7fdc94579090>\n"
     ]
    }
   ],
   "source": [
    "print(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1f876de42daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "output = model(input_ids[:4,:], token_type_ids=None, attention_mask=input_mask, labels=labels[:4,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logits'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_expand_inputs_for_generation',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_pad_token_id',\n",
       " '_get_resized_embeddings',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_sequence_length_for_generation',\n",
       " '_init_weights',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_input_ids_for_generation',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_update_seq_length_for_generation',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bert',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_output_embeddings',\n",
       " 'greedy_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_labels',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_token_embeddings',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sets the module in training mode.\n",
      "\n",
      "        This has any effect only on certain modules. See documentations of\n",
      "        particular modules for details of their behaviors in training/evaluation\n",
      "        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "        etc.\n",
      "\n",
      "        Args:\n",
      "            mode (bool): whether to set training mode (``True``) or evaluation\n",
      "                         mode (``False``). Default: ``True``.\n",
      "\n",
      "        Returns:\n",
      "            Module: self\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(model.train.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sets gradients of all model parameters to zero. See similar function\n",
      "        under :class:`torch.optim.Optimizer` for more context.\n",
      "\n",
      "        Arguments:\n",
      "            set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "                See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(model.zero_grad.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module._call_impl of BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
